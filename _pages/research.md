---
layout: archive
title: "Research Projects"
permalink: /research/
author_profile: true
---

{% include base_path %}


* Inference of Cosmic Initial Conditions Powered by Machine Learning
======

In general, the development of inference frameworks to study cosmic initial conditions is a daunting task. In this project, we considered a focus on the approach of leveraging cosmic Large-Scale Structure seen at extragalatic scales, aiming to use information encoded in these structures as an aid to understanding the initial conditions of the early Universe. Naturally, there are highly complex physics processes, many of them affecting each other, which would need to be considered for a proper analysis of the initial evolution of the Universe. As this is generally intractable, we start by looking at a simple proof-of-concept inference pipeline at small data scales, with the hope of guiding us towards more advanced approaches. Specifically, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. The specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we employ this novel machine learning approach by leveraging the hierarchical nature of structure formation.

Our aim is to use the hierarchical structure formation by representing the initial density field with different resolutions, with invertible transformations between these representations. Given that the representations are not fully independent, for example by considering the conservation of mass, we can effectively reduce our parameter space by several orders of magnitude, compared to inferring the initial density at a high resolution right away. In this post we skim over the details, but they can be found in the full thesis. 


The idea behind the training algorithm is to employ variational inference as a sampling approach, combined the the machine learning method known as normalizing flows. Specifically, the normalizing flow is trained to generate sample from the initial conditions from a specific cosmic structure configuration. Our aim is for our model to be given a target configuration, and then hopefully find a suitable initial density field for that configuration. In our test, we use first order Lagrangian perturbation theory as our gravity model, commonly referred to as the Zeldovich Approximation. Naturally, this is not a particularly complex gravity model and further investigation is of course warranted. 

We will employ variational inference by comparing the log-probability \\(\log q(\boldsymbol{z})\\) of the sample generated by our hierarchical model with an assigned target log-probability \\(\log p(\boldsymbol{z})\\). To compare the distributions, we use the so-called Kullback-Leibler divergence, defined by 

$$
    \text{KL}(q||p) = \int q(\boldsymbol{z}) \log \bigg( \frac{q(\boldsymbol{z})}{p(\boldsymbol{z})} \bigg) d\boldsymbol{z}.
$$

We cannot solve this integral analytically and have to resort to numerical approximations.

To approximate the divergence, we use a Monte Carlo approximation to above equation by drawing samples from the distribution \\(q(\boldsymbol{z})\\) given by the hierarchical model to replace the integral with a sum. We then obtain

$$
    \text{KL}(q||p) \approx \frac{1}{n}\sum_{i=1}^n [  \log q(\boldsymbol{z})-\log p(\boldsymbol{z})],
$$

where \\(n\\) is the number of samples. In our implementation, \\(n\\) is the batch size when training the model, meaning the amount of samples that are processed together as a batch in the machine learning model.

Denote the data instance we want to infer the initial conditions from by \\(\boldsymbol{d}\\), the dimension of the problem by \\(N\\) and the surrogate model by \\(\hat{f}\\). The target log-probability is given by


$$
    \mathcal{L}(\boldsymbol{z}, \lambda ) = -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \sum_{k=1}^N\bigg\{\bigg[ \frac{d_{ijk}-\hat{f}(\boldsymbol{z})}{\lambda}\bigg]^2 - z_{ijk}^2\bigg\},$$

where we assume a Gaussian likelihood.

In the flow chart below we see the rough idea of how the algorithm works. We start by generating a sample \\(\boldsymbol{x}\\) from our generative model, we run it through the simulation and then compare the result with the target data. Then, we adjust based on the above loss function. However, given that the simulation is non-differentiable, we replace the simulation with a machine learning model that emulates the gravity model, specifically an architecture known as a U-Net that makes use of convolutional layers and has been proven to be accurate at picking out features at different scales. With this machine learning surrogate model, we can effectively explore the parameter space due to the differentiability.


![flowchart](/images/flowchartVI.png)



Lastly, we showcase the results of a simple mock test on a 16x16x16 data. We first generate an initial Gaussian density field \\(\boldsymbol{x}_T\\), which we then run through our simulation to generate our target data \\(\boldsymbol{d}\\). The idea is to feed this target data into the algorithm and hope that it will be able to infer something close to the true initial condition. To quantify this, we use what is known as the cross-correlation.

To compare the correlation between the Fourier transform of two density fields \\(\delta_a\\) and \\(\delta_b\\), we calculate the cross power spectrum through 

$$
    \langle \delta_a(\boldsymbol{k}) \delta_b(\boldsymbol{k}')\rangle = \frac{(2\pi)^3}{V}\delta_D(\boldsymbol{k}+\boldsymbol{k}')P_{ab}(k)
$$

and define the cross-correlation coefficient as

$$
    C_k(\delta_a, \delta_b) = \frac{P_{ab}(k)}{\sqrt{P_a(k)P_b(k)}}.
$$

We now present the cross-correlation coefficient between \\(\boldsymbol{x}_T\\) and our inferred initial conditions \\(C_k(\boldsymbol{x}_T,\boldsymbol{x})\\) as well as the cross-correlation coefficient between the target data \\(\boldsymbol{d}\\) and simulation outputs \\(C_k(\boldsymbol{d}, \boldsymbol{y}_s)\\). In addition, we calculated the cross-correlation \\(C_k(\boldsymbol{d}, \boldsymbol{y}_U)\\) between the target data \\(\boldsymbol{d}\\) and U-net outputs \\(\boldsymbol{y}_U\\) and observed no significant deviation from the cross correlation using the simulation outputs. This suggests that the U-Net captures the same information as the simulation. We average the cross-correlation coefficient across 1000 samples and calculate the \\(1\sigma\\) error and present the results in the below Figure.


![cross_corr](/images/cross_corr.png)

We note a high correlation for smaller k-values, which correspond to larger scales. As we run the data sample through the simulation, we see the cross correlation being pushed up, corresponding to larger regions in the initial conditions collapsing into smaller more dense regions. The behaviour of the cross-correlation is as expected from similar and more advanced simulation, like seen in this paper, albeit this cross correlation does not have a higher enough correlation for it to be conclusive. Further testing with larger data sizes and more realistic simulations need to be performed to give a more definitive answer into the viablility of this approach, although this proof-of-concept showcases promise.