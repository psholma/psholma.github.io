---
title: "Research Projects"
permalink: /research/
author_profile: true
---

{% include base_path %}


* Inference of Cosmic Initial Conditions Powered by Machine Learning
======

In general, the development of inference frameworks to study cosmic initial conditions is a daunting task. This question is especially relevant to investigate with the advent of next-generation galaxy surveys the coming decade, which will generate enormous amounts of data. In this project, we considered a focus on the approach of leveraging cosmic Large-Scale Structure seen at extragalatic scales, aiming to use information encoded in these structures as an aid to understanding the initial conditions of the early Universe. Naturally, there are highly complex physics processes, many of them affecting each other, which would need to be considered for a proper analysis of the initial evolution of the Universe. As this is generally intractable, we start by looking at a simple proof-of-concept inference pipeline at small data scales, with the hope of guiding us towards more advanced approaches. Specifically, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. The specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we employ this novel machine learning approach by leveraging the hierarchical nature of structure formation.

Our aim is to use the hierarchical structure formation by representing the initial density field with different resolutions, with invertible transformations between these representations. Given that the representations are not fully independent, for example by considering the conservation of mass, we can effectively reduce our parameter space by several orders of magnitude, compared to inferring the initial density at a high resolution right away. In this post we skim over the details, but they can be found in the full thesis. 


The idea behind the training algorithm is to employ variational inference as a sampling approach, combined with the machine learning method known as normalizing flows. Specifically, the normalizing flow is trained to generate samples for the initial conditions from a specific cosmic structure configuration. Our aim is for the model to be given a target configuration, and then hopefully find a suitable initial density field for that configuration through exploring the parameter space of the model. In our test, we use first order Lagrangian perturbation theory as our gravity model, commonly referred to as the Zeldovich Approximation. Naturally, this is not a particularly complex gravity model and further investigation is, of course, warranted. 

We will employ variational inference by comparing the log-probability \\(\log q(\boldsymbol{z})\\) of the sample generated by our hierarchical model with an assigned target log-probability \\(\log p(\boldsymbol{z})\\). To compare the distributions, we use the so-called Kullback-Leibler divergence, defined by 

$$
    \text{KL}(q||p) = \int q(\boldsymbol{z}) \log \bigg( \frac{q(\boldsymbol{z})}{p(\boldsymbol{z})} \bigg) d\boldsymbol{z}.
$$

We cannot solve this integral analytically and resort to numerical approximations.

To approximate the divergence, we use a Monte Carlo approximation to above equation by drawing samples from the distribution \\(q(\boldsymbol{z})\\) given by the hierarchical model to replace the integral with a sum. We then obtain

$$
    \text{KL}(q||p) \approx \frac{1}{n}\sum_{i=1}^n [  \log q(\boldsymbol{z})-\log p(\boldsymbol{z})],
$$

where \\(n\\) is the number of samples. In our implementation, \\(n\\) is the batch size when training the model, meaning the amount of samples that are processed together as a batch in the machine learning model.

Denote the data instance we want to infer the initial conditions from by \\(\boldsymbol{d}\\), the dimension of the problem by \\(N\\) and the surrogate model by \\(\hat{f}\\). The target log-probability is given by


$$
    \mathcal{L}(\boldsymbol{z}, \lambda ) = -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \sum_{k=1}^N\bigg\{\bigg[ \frac{d_{ijk}-\hat{f}(\boldsymbol{z})}{\lambda}\bigg]^2 - z_{ijk}^2\bigg\},$$

where we assume a Gaussian likelihood.

In the flow chart below we see the rough idea of how the algorithm works. We start by generating a sample \\(\boldsymbol{x}\\) from our generative model, we run it through the simulation and then compare the result with the target data. Then, we adjust based on the above loss function. However, given that the simulation is non-differentiable, we replace the simulation with a machine learning model that emulates the gravity model, specifically an architecture known as a U-Net that makes use of convolutional layers and has been proven to be accurate at picking out features at different scales. With this machine learning surrogate model, we can effectively explore the parameter space due to the differentiability.


<!--![flowchart](/images/flowchartVI.png#size)-->
<img src="/images/flowchartVI.png" class="center"/>
<div class="caption">
<p> Figure 1: Algorithmic flow chart of the learning process </p>
</div>



Lastly, we showcase the results of a simple mock test on a 16x16x16 data. We first generate an initial Gaussian density field \\(\boldsymbol{x}_T\\), which we then run through our simulation to generate our target data \\(\boldsymbol{d}\\). The idea is to feed this target data into the algorithm and hope that it will be able to infer something close to the true initial condition. To quantify this, we use what is known as the cross-correlation.

To compare the correlation between the Fourier transform of two density fields \\(\delta_a\\) and \\(\delta_b\\), we calculate the cross power spectrum through 

$$
    \langle \delta_a(\boldsymbol{k}) \delta_b(\boldsymbol{k}')\rangle = \frac{(2\pi)^3}{V}\delta_D(\boldsymbol{k}+\boldsymbol{k}')P_{ab}(k)
$$

and define the cross-correlation coefficient as

$$
    C_k(\delta_a, \delta_b) = \frac{P_{ab}(k)}{\sqrt{P_a(k)P_b(k)}}.
$$

We now present the cross-correlation coefficient between \\(\boldsymbol{x}_T\\) and our inferred initial conditions \\(C_k(\boldsymbol{x}_T,\boldsymbol{x})\\) as well as the cross-correlation coefficient between the target data \\(\boldsymbol{d}\\) and simulation outputs \\(C_k(\boldsymbol{d}, \boldsymbol{y}_s)\\). In addition, we calculated the cross-correlation \\(C_k(\boldsymbol{d}, \boldsymbol{y}_U)\\) between the target data \\(\boldsymbol{d}\\) and U-net outputs \\(\boldsymbol{y}_U\\) and observed no significant deviation from the cross correlation using the simulation outputs. This suggests that the U-Net captures the same information as the simulation. We average the cross-correlation coefficient across 1000 samples and calculate the \\(1\sigma\\) error and present the results in the below Figure.


<!--![cross_corr](/images/cross_corr.png "Cross-Correlation")-->

<img src="/images/cross_corr.png" class="center"/>
<div class="caption">
<p> Figure 2: Cross-correlation comparisons of generated samples with the ground truth </p>
</div>

We note a high correlation for smaller k-values, which correspond to larger scales. As we run the data sample through the simulation, we see the cross correlation being pushed up, corresponding to larger regions in the initial conditions collapsing into smaller more dense regions. The behaviour of the cross-correlation is as expected from more advanced simulation, such as the analysis seen in this [paper](https://arxiv.org/abs/2312.09271), albeit the cross correlation in the above figure does not have a high enough cross-correlation across all scales for the test to be conclusive on the scientific viability of the method. Further testing with larger data sizes and more realistic simulations need to be performed to give a more definitive answer into the viablility of this approach, although this proof-of-concept showcases promise.

* Challenges and Outlook
======

Some of the challenges include the minimization of the KL-divergence, as it can be prone to get stuck in local optimum in the training process. One method we used to counter this was to impose regularization by discouraging parameter values far away from the expected behaviour of the initial density field, namely that inflation theory predicts a Gaussian density field.

The tuning of the surrogate machine learning model determines a lot of consideration into finding an appropriate architecture, in addition to training consideration of hyperparameters, size of the data training set and so on. 

The tests performed in the thesis are on very small data arrays and we need to push up the data dimensions for the method to showcase scientific promise, which naturally will complicate the process due to increase computational demand. Furthermore, we might need to add additional layers into the normalizing flow model.